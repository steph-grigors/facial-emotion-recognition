# ========================================================================
# Enhanced CNN Configuration (ResNet50)
# ========================================================================
# ResNet50 with transfer learning that achieved ~80% accuracy
# Input: 128Ã—128 RGB
# Architecture: ResNet50 pretrained + custom classifier
# Parameters: ~23.5M (only ~3K trainable initially)
# ========================================================================

project:
  name: "facial-emotion-recognition"
  version: "1.0.0"
  description: "Enhanced CNN with ResNet50 backbone for emotion recognition"

# ========================================================================
# DATA CONFIGURATION
# ========================================================================

data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  augmented_dir: "data/augmented"

  # Dataset splits
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

  # Image specifications (matches EnhancedCNN)
  image_size: [128, 128]  # Height, Width (smaller than baseline!)
  channels: 3  # RGB
  normalize_mean: [0.485, 0.456, 0.406]  # ImageNet normalization
  normalize_std: [0.229, 0.224, 0.225]

  # Classes
  classes:
    - "angry"
    - "disgust"
    - "fear"
    - "happy"
    - "neutral"
    - "sad"
    - "surprise"
  
  num_classes: 7

  # Data augmentation (stronger for enhanced model)
  augmentation:
    enabled: true
    random_horizontal_flip: 0.5
    random_rotation: 15
    color_jitter:
      brightness: 0.3
      contrast: 0.3
      saturation: 0.2
      hue: 0.1
    random_affine:
      degrees: 10
      translate: [0.1, 0.1]
      scale: [0.8, 1.2]
      shear: 5
    random_erasing:
      enabled: true
      p: 0.3
      scale: [0.02, 0.2]
    # Advanced augmentation
    mixup:
      enabled: false  # Can enable for better generalization
      alpha: 0.2
    cutmix:
      enabled: false
      alpha: 1.0

# ========================================================================
# MODEL CONFIGURATION
# ========================================================================

model:
  architecture: "enhanced_cnn"  # Uses ResNet50
  num_classes: 7
  
  # Transfer learning settings
  pretrained: true  # Load ImageNet weights
  freeze_backbone: true  # Freeze ResNet50 initially
  
  # Classifier settings
  dropout_rate: 0.5
  
  # Two-phase training strategy:
  # Phase 1 (epochs 1-20): Freeze backbone, train only classifier
  # Phase 2 (epochs 21-50): Unfreeze backbone, fine-tune all layers
  
  unfreeze_after_epoch: 20  # Unfreeze backbone after this epoch

# ========================================================================
# TRAINING CONFIGURATION
# ========================================================================

training:
  # Basic settings
  epochs: 50
  batch_size: 64  # Can increase to 128 with smaller image size
  num_workers: 4
  pin_memory: true

  # Optimization (Phase 1: Frozen backbone)
  optimizer: "adamw"
  learning_rate: 0.001  # Higher LR for classifier training
  weight_decay: 0.01
  
  # Phase 2 settings (after unfreezing)
  fine_tune_learning_rate: 0.0001  # Lower LR for fine-tuning
  fine_tune_weight_decay: 0.01

  # Learning rate scheduler
  scheduler:
    type: "reduce_on_plateau"
    mode: "min"
    patience: 5
    factor: 0.5
    min_lr: 0.000001
    
  # Warmup (helps with transfer learning)
  warmup:
    enabled: true
    warmup_epochs: 3
    warmup_lr: 0.0001

  # Loss function
  loss:
    type: "cross_entropy"
    label_smoothing: 0.1  # Helps with generalization
    # Optional: Use class weights for imbalanced dataset
    # class_weights: "auto"  # Uncomment to compute from dataset

  # Early stopping
  early_stopping:
    enabled: true
    patience: 15  # More patience for two-phase training
    min_delta: 0.001
    monitor: "val_loss"

  # Gradient clipping
  gradient_clipping:
    enabled: true
    max_norm: 1.0

  # Mixed precision training (faster training, less memory)
  mixed_precision: true

  # Checkpointing
  checkpoint:
    save_best_only: true
    save_frequency: 5  # Save every 5 epochs
    monitor: "val_accuracy"
    mode: "max"
    
  # Save checkpoint after phase 1 (before unfreezing)
  save_phase1_checkpoint: true

# ========================================================================
# VALIDATION & TESTING
# ========================================================================

evaluation:
  batch_size: 128  # Larger batch for validation (no backprop)
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "confusion_matrix"
  
  # Test-time augmentation (optional, improves accuracy)
  tta:
    enabled: false  # Set to true for +1-2% accuracy boost
    num_augmentations: 5

# ========================================================================
# LOGGING
# ========================================================================

logging:
  log_dir: "logs/enhanced_cnn"
  tensorboard: true
  log_frequency: 10  # Log every 10 batches
  save_confusion_matrix: true
  save_plots: true
  
  # Log learning rate changes
  log_lr: true
  
  # Log model architecture at start
  log_model_graph: true

# ========================================================================
# PATHS
# ========================================================================

paths:
  data_dir: "data/processed"
  models_dir: "models"
  checkpoints_dir: "models/checkpoints"
  final_models_dir: "models/final"
  results_dir: "results"
  logs_dir: "logs"

# ========================================================================
# REPRODUCIBILITY
# ========================================================================

random_seed: 42
deterministic: true

# ========================================================================
# HARDWARE
# ========================================================================

device: "cuda"  # Use "cpu" if no GPU available
gpu_ids: [0]

# ========================================================================
# ADVANCED TRAINING STRATEGIES
# ========================================================================

# Two-phase training strategy
two_phase_training:
  enabled: true
  
  # Phase 1: Train classifier only (frozen backbone)
  phase1:
    epochs: 20
    learning_rate: 0.001
    freeze_backbone: true
    description: "Train only the classifier head"
  
  # Phase 2: Fine-tune entire model (unfrozen backbone)
  phase2:
    epochs: 30
    learning_rate: 0.0001
    freeze_backbone: false
    description: "Fine-tune all layers with lower learning rate"

# ========================================================================
# EXPECTED PERFORMANCE
# ========================================================================

# Expected results:
# Phase 1 (frozen backbone):
# - Training accuracy: ~75-78%
# - Validation accuracy: ~72-75%
#
# Phase 2 (fine-tuned):
# - Training accuracy: ~85-88%
# - Validation accuracy: ~78-82%
# - Test accuracy: ~78-80%
#
# Performance metrics:
# - Training time: ~8-12 min/epoch on GPU (faster due to smaller images)
# - Total parameters: ~23.5M
# - Trainable (Phase 1): ~3K (just classifier)
# - Trainable (Phase 2): ~23.5M (all layers)
# - Model size: ~91 MB
# - Inference time: ~80ms on GPU, ~500ms on CPU
#
# Best practices:
# - Use batch_size=128 if you have enough GPU memory
# - Enable TTA during final evaluation for +1-2% accuracy
# - Consider mixup/cutmix for additional regularization
# - Monitor both phases - Phase 1 should reach ~75% quickly
