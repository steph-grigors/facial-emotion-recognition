# Main Configuration File for Facial Emotion Recognition Project
# This file contains all configurable parameters for the project

project:
  name: "facial-emotion-recognition"
  version: "1.0.0"
  author: "Your Name"
  description: "Deep Learning model for recognizing facial emotions"

# Data Configuration
data:
  raw_dir: "data/raw"
  processed_dir: "data/processed"
  augmented_dir: "data/augmented"

  # Dataset splits
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

  # Image specifications
  image_size: [224, 224]  # Height, Width
  channels: 3  # RGB
  normalize_mean: [0.485, 0.456, 0.406]  # ImageNet means
  normalize_std: [0.229, 0.224, 0.225]   # ImageNet stds

  # Classes (update based on your dataset)
  classes:
    - "angry"
    - "disgust"
    - "fear"
    - "happy"
    - "neutral"
    - "sad"
    - "surprise"

  num_classes: 7

  # Data augmentation
  augmentation:
    enabled: true
    random_horizontal_flip: 0.5
    random_rotation: 15
    brightness: 0.2
    contrast: 0.2
    random_crop: true
    cutout: false
    mixup: false
    mixup_alpha: 0.2

# Model Configuration
model:
  architecture: "resnet50"  # Options: resnet50, efficientnet_b0, custom_cnn, vgg16
  pretrained: true
  freeze_backbone: false  # Whether to freeze pretrained weights initially
  dropout_rate: 0.5

  # Custom CNN settings (if using custom architecture)
  custom_cnn:
    conv_layers: [32, 64, 128, 256]
    kernel_size: 3
    pool_size: 2
    dense_layers: [512, 256]

# Training Configuration
training:
  # Basic settings
  epochs: 100
  batch_size: 32
  num_workers: 4
  pin_memory: true

  # Optimization
  optimizer: "adamw"  # Options: adam, adamw, sgd
  learning_rate: 0.001
  weight_decay: 0.0001
  momentum: 0.9  # For SGD

  # Learning rate scheduler
  scheduler:
    type: "reduce_on_plateau"  # Options: reduce_on_plateau, cosine, step, exponential
    patience: 5
    factor: 0.5
    min_lr: 0.000001
    warmup_epochs: 5

  # Loss function
  loss:
    type: "cross_entropy"  # Options: cross_entropy, focal_loss, label_smoothing
    label_smoothing: 0.1
    focal_loss_gamma: 2.0
    class_weights: null  # Set to "auto" to compute from dataset

  # Early stopping
  early_stopping:
    enabled: true
    patience: 15
    min_delta: 0.001
    monitor: "val_loss"  # Options: val_loss, val_accuracy

  # Gradient clipping
  gradient_clipping:
    enabled: true
    max_norm: 1.0

  # Mixed precision training
  mixed_precision: true

  # Checkpointing
  checkpoint:
    save_best_only: true
    save_frequency: 5  # Save every N epochs
    monitor: "val_accuracy"
    mode: "max"  # max for accuracy, min for loss

# Validation & Testing
evaluation:
  batch_size: 64
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "confusion_matrix"
    - "roc_auc"

  # Test-time augmentation
  tta:
    enabled: false
    num_augmentations: 5

# Inference Configuration
inference:
  batch_size: 1
  device: "cuda"  # cuda or cpu
  confidence_threshold: 0.5
  ensemble_models: []  # List of model paths for ensemble

# Logging & Experiment Tracking
logging:
  log_dir: "experiments/runs"
  tensorboard: true
  wandb:
    enabled: false
    project: "facial-emotion-recognition"
    entity: null  # Your wandb username
  mlflow:
    enabled: false
    tracking_uri: "mlruns"

  # What to log
  log_images: true
  log_frequency: 10  # Log every N batches
  save_confusion_matrix: true
  save_misclassified: true

# Paths
paths:
  data_dir: "data"
  models_dir: "models"
  checkpoints_dir: "models/checkpoints"
  final_models_dir: "models/final"
  results_dir: "results"
  logs_dir: "logs"

# Reproducibility
random_seed: 42
deterministic: true  # For reproducible results (may impact performance)

# Hardware
device: "cuda"  # cuda or cpu
gpu_ids: [0]  # List of GPU IDs to use
distributed: false  # Distributed training

# Debugging
debug:
  enabled: false
  overfit_single_batch: false
  fast_dev_run: false  # Run 1 batch for sanity check
  profile: false
