# EfficientNet-B0 Configuration
# State-of-the-art efficient architecture

model:
  name: "efficientnet_b0"
  architecture: "efficientnet_b0"
  pretrained: true
  pretrained_weights: "IMAGENET1K_V1"

  # Model modifications
  freeze_backbone: true
  unfreeze_after_epoch: 8
  num_classes: 7
  dropout_rate: 0.4

  # Custom head
  custom_head:
    - type: "adaptive_avg_pool"
    - type: "flatten"
    - type: "dropout"
      rate: 0.4
    - type: "linear"
      units: 512
    - type: "relu"
    - type: "batch_norm"
    - type: "dropout"
      rate: 0.3
    - type: "linear"
      units: 7

# Training configuration
training:
  epochs: 100
  batch_size: 48  # EfficientNet is memory efficient

  # Multi-phase training
  phase1:  # Warmup with frozen backbone
    epochs: 8
    learning_rate: 0.001
    optimizer: "adam"

  phase2:  # Fine-tune entire network
    epochs: 92
    learning_rate: 0.0001
    optimizer: "adamw"
    weight_decay: 0.01

  # Advanced scheduler
  scheduler:
    type: "cosine_with_warmup"
    warmup_epochs: 5
    t_max: 100
    eta_min: 0.0000001

  # Loss function with class balancing
  loss:
    type: "focal_loss"
    gamma: 2.0
    alpha: null  # Auto-compute from class distribution

  # Optimization techniques
  mixed_precision: true
  gradient_accumulation_steps: 2  # Effective batch size = 96

  gradient_clipping:
    enabled: true
    max_norm: 1.0

  # Early stopping
  early_stopping:
    patience: 25
    min_delta: 0.0001

# Data configuration
data:
  image_size: [224, 224]  # EfficientNet-B0 default

  # Strong augmentation for robustness
  augmentation:
    enabled: true

    # Geometric augmentations
    random_horizontal_flip: 0.5
    random_rotation: 25
    random_affine: true
    random_perspective: 0.1

    # Color augmentations
    color_jitter:
      brightness: 0.3
      contrast: 0.3
      saturation: 0.2
      hue: 0.1

    # Advanced augmentations
    random_erasing: 0.3
    cutout:
      enabled: true
      num_holes: 1
      size: 16

    # Normalization (EfficientNet specific)
    normalize:
      mean: [0.485, 0.456, 0.406]
      std: [0.229, 0.224, 0.225]

# Regularization
regularization:
  dropout_rate: 0.4
  weight_decay: 0.01
  label_smoothing: 0.1
  stochastic_depth: 0.2  # Drop path for EfficientNet

# Expected performance
expected_performance:
  train_accuracy: null
  val_accuracy: null
  test_accuracy: null
  params_millions: 5.3
  inference_time_ms: null
  notes: "EfficientNet should provide best accuracy/efficiency trade-off"
