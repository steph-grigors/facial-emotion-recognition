{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≠ Facial Emotion Recognition - Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Purpose**: Understand the dataset before building models\n",
    "\n",
    "**What we'll explore**:\n",
    "1. Dataset structure and organization\n",
    "2. Class distribution (balanced or imbalanced?)\n",
    "3. Image properties (size, quality, format)\n",
    "4. Visual inspection of samples\n",
    "5. Data quality issues\n",
    "6. Insights for preprocessing and training\n",
    "\n",
    "**Date**: `YYYY-MM-DD`  \n",
    "**Author**: Your Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T13:12:04.185639Z",
     "iopub.status.busy": "2025-11-12T13:12:04.185176Z",
     "iopub.status.idle": "2025-11-12T13:12:06.819541Z",
     "shell.execute_reply": "2025-11-12T13:12:06.818421Z",
     "shell.execute_reply.started": "2025-11-12T13:12:04.185602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è 1. Dataset Structure & Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "DATA_DIR = Path('../data/raw')  # Adjust this path based on your setup\n",
    "\n",
    "# Check if data directory exists\n",
    "if not DATA_DIR.exists():\n",
    "    print(f\"‚ùå Data directory not found: {DATA_DIR}\")\n",
    "    print(\"Please download the dataset first using: make data\")\n",
    "else:\n",
    "    print(f\"‚úÖ Data directory found: {DATA_DIR}\")\n",
    "    \n",
    "# List all subdirectories\n",
    "subdirs = [d for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "print(f\"\\nüìÅ Found {len(subdirs)} subdirectories:\")\n",
    "for subdir in sorted(subdirs):\n",
    "    print(f\"   - {subdir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore directory structure in detail\n",
    "def explore_directory_structure(root_path):\n",
    "    \"\"\"Recursively explore directory structure\"\"\"\n",
    "    structure = {}\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        rel_path = Path(dirpath).relative_to(root_path)\n",
    "        structure[str(rel_path)] = {\n",
    "            'num_subdirs': len(dirnames),\n",
    "            'num_files': len(filenames),\n",
    "            'subdirs': dirnames,\n",
    "            'file_extensions': list(set([Path(f).suffix for f in filenames]))\n",
    "        }\n",
    "    \n",
    "    return structure\n",
    "\n",
    "structure = explore_directory_structure(DATA_DIR)\n",
    "\n",
    "print(\"\\nüìä Directory Structure Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "for path, info in list(structure.items())[:10]:  # Show first 10\n",
    "    print(f\"\\n{path}:\")\n",
    "    print(f\"  Subdirectories: {info['num_subdirs']}\")\n",
    "    print(f\"  Files: {info['num_files']}\")\n",
    "    if info['subdirs']:\n",
    "        print(f\"  Contains: {', '.join(info['subdirs'][:5])}\")\n",
    "    if info['file_extensions']:\n",
    "        print(f\"  File types: {', '.join(info['file_extensions'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 2. Dataset Statistics & Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emotion classes (adjust based on your dataset)\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "def collect_dataset_info(data_path, splits=['train', 'test', 'validation']):\n",
    "    \"\"\"Collect comprehensive dataset information\"\"\"\n",
    "    \n",
    "    dataset_info = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        split_path = data_path / split\n",
    "        \n",
    "        if not split_path.exists():\n",
    "            print(f\"‚ö†Ô∏è  Split '{split}' not found at {split_path}\")\n",
    "            continue\n",
    "        \n",
    "        split_info = {'total': 0, 'classes': {}}\n",
    "        \n",
    "        for emotion in EMOTIONS:\n",
    "            emotion_path = split_path / emotion\n",
    "            \n",
    "            if emotion_path.exists():\n",
    "                images = list(emotion_path.glob('*.jpg')) + \\\n",
    "                        list(emotion_path.glob('*.png')) + \\\n",
    "                        list(emotion_path.glob('*.jpeg'))\n",
    "                \n",
    "                count = len(images)\n",
    "                split_info['classes'][emotion] = count\n",
    "                split_info['total'] += count\n",
    "            else:\n",
    "                split_info['classes'][emotion] = 0\n",
    "        \n",
    "        dataset_info[split] = split_info\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Collect information\n",
    "dataset_info = collect_dataset_info(DATA_DIR)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for split, info in dataset_info.items():\n",
    "    print(f\"\\n{split.upper()} SET: {info['total']:,} images\")\n",
    "    print(\"-\" * 40)\n",
    "    for emotion, count in info['classes'].items():\n",
    "        percentage = (count / info['total'] * 100) if info['total'] > 0 else 0\n",
    "        print(f\"  {emotion:12s}: {count:5,} ({percentage:5.2f}%)\")\n",
    "\n",
    "# Calculate total\n",
    "total_images = sum(info['total'] for info in dataset_info.values())\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"TOTAL DATASET: {total_images:,} images\")\n",
    "print(f\"{'='*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive DataFrame for easier analysis\n",
    "data_records = []\n",
    "\n",
    "for split, info in dataset_info.items():\n",
    "    for emotion, count in info['classes'].items():\n",
    "        data_records.append({\n",
    "            'split': split,\n",
    "            'emotion': emotion,\n",
    "            'count': count,\n",
    "            'percentage': (count / info['total'] * 100) if info['total'] > 0 else 0\n",
    "        })\n",
    "\n",
    "df_stats = pd.DataFrame(data_records)\n",
    "\n",
    "print(\"\\nüìä Dataset Statistics DataFrame:\")\n",
    "print(df_stats.head(10))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìà Summary by Split:\")\n",
    "print(df_stats.groupby('split')['count'].agg(['sum', 'mean', 'std', 'min', 'max']))\n",
    "\n",
    "print(\"\\nüìà Summary by Emotion (across all splits):\")\n",
    "print(df_stats.groupby('emotion')['count'].agg(['sum', 'mean', 'std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 3. Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of class distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Class Distribution Analysis', fontsize=20, fontweight='bold')\n",
    "\n",
    "# 1. Bar plot by split\n",
    "ax1 = axes[0, 0]\n",
    "df_pivot = df_stats.pivot(index='emotion', columns='split', values='count')\n",
    "df_pivot.plot(kind='bar', ax=ax1, width=0.8)\n",
    "ax1.set_title('Images per Emotion by Split', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Emotion', fontsize=12)\n",
    "ax1.set_ylabel('Number of Images', fontsize=12)\n",
    "ax1.legend(title='Split', fontsize=10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. Pie chart - overall distribution\n",
    "ax2 = axes[0, 1]\n",
    "emotion_totals = df_stats.groupby('emotion')['count'].sum()\n",
    "colors = sns.color_palette('husl', len(emotion_totals))\n",
    "wedges, texts, autotexts = ax2.pie(emotion_totals, labels=emotion_totals.index, \n",
    "                                     autopct='%1.1f%%', startangle=90, colors=colors)\n",
    "ax2.set_title('Overall Emotion Distribution', fontsize=14, fontweight='bold')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(10)\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "# 3. Stacked bar chart\n",
    "ax3 = axes[1, 0]\n",
    "df_pivot_pct = df_pivot.div(df_pivot.sum(axis=1), axis=0) * 100\n",
    "df_pivot_pct.plot(kind='bar', stacked=True, ax=ax3, width=0.8)\n",
    "ax3.set_title('Percentage Distribution by Split', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Emotion', fontsize=12)\n",
    "ax3.set_ylabel('Percentage', fontsize=12)\n",
    "ax3.legend(title='Split', fontsize=10)\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 4. Box plot - distribution across splits\n",
    "ax4 = axes[1, 1]\n",
    "df_stats.boxplot(column='count', by='split', ax=ax4)\n",
    "ax4.set_title('Distribution of Class Sizes by Split', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Split', fontsize=12)\n",
    "ax4.set_ylabel('Number of Images', fontsize=12)\n",
    "plt.suptitle('')  # Remove auto title from boxplot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üíæ Saved visualization to: ../results/plots/class_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for class imbalance\n",
    "print(\"\\n‚öñÔ∏è  CLASS IMBALANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for split, info in dataset_info.items():\n",
    "    counts = list(info['classes'].values())\n",
    "    if counts:\n",
    "        max_count = max(counts)\n",
    "        min_count = min(counts)\n",
    "        imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "        \n",
    "        print(f\"\\n{split.upper()} Set:\")\n",
    "        print(f\"  Max class size: {max_count:,}\")\n",
    "        print(f\"  Min class size: {min_count:,}\")\n",
    "        print(f\"  Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "        \n",
    "        if imbalance_ratio > 3:\n",
    "            print(\"  ‚ö†Ô∏è  SIGNIFICANT IMBALANCE - Consider using:\")\n",
    "            print(\"     - Class weights\")\n",
    "            print(\"     - Focal loss\")\n",
    "            print(\"     - Oversampling minority classes\")\n",
    "        elif imbalance_ratio > 1.5:\n",
    "            print(\"  ‚ö†Ô∏è  MODERATE IMBALANCE - Monitor class-wise metrics\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ WELL BALANCED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è 4. Image Properties Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image properties (sample from each class)\n",
    "def analyze_image_properties(data_path, split='train', samples_per_class=50):\n",
    "    \"\"\"Analyze dimensions, file sizes, and formats\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for emotion in EMOTIONS:\n",
    "        emotion_path = data_path / split / emotion\n",
    "        \n",
    "        if not emotion_path.exists():\n",
    "            continue\n",
    "        \n",
    "        # Get sample of images\n",
    "        images = list(emotion_path.glob('*.jpg')) + \\\n",
    "                list(emotion_path.glob('*.png')) + \\\n",
    "                list(emotion_path.glob('*.jpeg'))\n",
    "        \n",
    "        sample = np.random.choice(images, min(samples_per_class, len(images)), replace=False)\n",
    "        \n",
    "        for img_path in sample:\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                width, height = img.size\n",
    "                channels = len(img.getbands())\n",
    "                file_size = os.path.getsize(img_path) / 1024  # KB\n",
    "                \n",
    "                results.append({\n",
    "                    'emotion': emotion,\n",
    "                    'width': width,\n",
    "                    'height': height,\n",
    "                    'aspect_ratio': width / height,\n",
    "                    'channels': channels,\n",
    "                    'total_pixels': width * height,\n",
    "                    'file_size_kb': file_size,\n",
    "                    'format': img.format\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"üîç Analyzing image properties (this may take a moment)...\")\n",
    "df_images = analyze_image_properties(DATA_DIR, split='train', samples_per_class=100)\n",
    "\n",
    "print(\"\\nüìä Image Properties Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total images analyzed: {len(df_images):,}\")\n",
    "print(f\"\\nDimensions:\")\n",
    "print(f\"  Width:  {df_images['width'].min():.0f} - {df_images['width'].max():.0f} px (mean: {df_images['width'].mean():.1f})\")\n",
    "print(f\"  Height: {df_images['height'].min():.0f} - {df_images['height'].max():.0f} px (mean: {df_images['height'].mean():.1f})\")\n",
    "print(f\"  Aspect ratio: {df_images['aspect_ratio'].min():.2f} - {df_images['aspect_ratio'].max():.2f} (mean: {df_images['aspect_ratio'].mean():.2f})\")\n",
    "print(f\"\\nFile properties:\")\n",
    "print(f\"  File size: {df_images['file_size_kb'].min():.1f} - {df_images['file_size_kb'].max():.1f} KB (mean: {df_images['file_size_kb'].mean():.1f} KB)\")\n",
    "print(f\"  Channels: {df_images['channels'].value_counts().to_dict()}\")\n",
    "print(f\"  Formats: {df_images['format'].value_counts().to_dict()}\")\n",
    "\n",
    "# Detailed statistics\n",
    "print(\"\\nüìà Detailed Statistics:\")\n",
    "print(df_images[['width', 'height', 'aspect_ratio', 'file_size_kb']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image properties\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Image Properties Analysis', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 1. Width distribution\n",
    "axes[0, 0].hist(df_images['width'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(df_images['width'].mean(), color='red', linestyle='--', label=f\"Mean: {df_images['width'].mean():.0f}\")\n",
    "axes[0, 0].set_title('Width Distribution')\n",
    "axes[0, 0].set_xlabel('Width (pixels)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Height distribution\n",
    "axes[0, 1].hist(df_images['height'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].axvline(df_images['height'].mean(), color='red', linestyle='--', label=f\"Mean: {df_images['height'].mean():.0f}\")\n",
    "axes[0, 1].set_title('Height Distribution')\n",
    "axes[0, 1].set_xlabel('Height (pixels)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Aspect ratio\n",
    "axes[0, 2].hist(df_images['aspect_ratio'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 2].axvline(df_images['aspect_ratio'].mean(), color='red', linestyle='--', label=f\"Mean: {df_images['aspect_ratio'].mean():.2f}\")\n",
    "axes[0, 2].set_title('Aspect Ratio Distribution')\n",
    "axes[0, 2].set_xlabel('Aspect Ratio (W/H)')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(alpha=0.3)\n",
    "\n",
    "# 4. File size\n",
    "axes[1, 0].hist(df_images['file_size_kb'], bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 0].axvline(df_images['file_size_kb'].mean(), color='red', linestyle='--', label=f\"Mean: {df_images['file_size_kb'].mean():.1f} KB\")\n",
    "axes[1, 0].set_title('File Size Distribution')\n",
    "axes[1, 0].set_xlabel('File Size (KB)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 5. Scatter: width vs height\n",
    "scatter = axes[1, 1].scatter(df_images['width'], df_images['height'], \n",
    "                             c=df_images['emotion'].astype('category').cat.codes, \n",
    "                             alpha=0.5, cmap='tab10')\n",
    "axes[1, 1].plot([0, max(df_images['width'].max(), df_images['height'].max())],\n",
    "                [0, max(df_images['width'].max(), df_images['height'].max())], \n",
    "                'r--', alpha=0.5, label='Square (1:1)')\n",
    "axes[1, 1].set_title('Width vs Height')\n",
    "axes[1, 1].set_xlabel('Width (pixels)')\n",
    "axes[1, 1].set_ylabel('Height (pixels)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "# 6. Box plot by emotion\n",
    "df_images.boxplot(column='total_pixels', by='emotion', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Image Size by Emotion')\n",
    "axes[1, 2].set_xlabel('Emotion')\n",
    "axes[1, 2].set_ylabel('Total Pixels')\n",
    "plt.setp(axes[1, 2].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/image_properties.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üíæ Saved visualization to: ../results/plots/image_properties.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 5. Visual Inspection of Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each emotion class\n",
    "def display_emotion_samples(data_path, split='train', samples_per_emotion=5):\n",
    "    \"\"\"Display random samples from each emotion class\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(len(EMOTIONS), samples_per_emotion, \n",
    "                            figsize=(samples_per_emotion * 3, len(EMOTIONS) * 3))\n",
    "    fig.suptitle(f'Sample Images from {split.upper()} Set', \n",
    "                fontsize=20, fontweight='bold', y=0.995)\n",
    "    \n",
    "    for i, emotion in enumerate(EMOTIONS):\n",
    "        emotion_path = data_path / split / emotion\n",
    "        \n",
    "        if not emotion_path.exists():\n",
    "            continue\n",
    "        \n",
    "        # Get random samples\n",
    "        images = list(emotion_path.glob('*.jpg')) + \\\n",
    "                list(emotion_path.glob('*.png')) + \\\n",
    "                list(emotion_path.glob('*.jpeg'))\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "            \n",
    "        sample = np.random.choice(images, min(samples_per_emotion, len(images)), replace=False)\n",
    "        \n",
    "        for j, img_path in enumerate(sample):\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                \n",
    "                if len(axes.shape) == 1:\n",
    "                    ax = axes[j]\n",
    "                else:\n",
    "                    ax = axes[i, j]\n",
    "                \n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "                \n",
    "                # Add label on the first column\n",
    "                if j == 0:\n",
    "                    ax.set_ylabel(emotion.upper(), fontsize=14, fontweight='bold', rotation=0, \n",
    "                                 ha='right', va='center')\n",
    "                \n",
    "                # Add image info\n",
    "                ax.set_title(f\"{img.size[0]}x{img.size[1]}\", fontsize=9)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/plots/emotion_samples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Saved visualization to: ../results/plots/emotion_samples.png\")\n",
    "\n",
    "display_emotion_samples(DATA_DIR, split='train', samples_per_emotion=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® 6. Image Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze brightness, contrast, and color distribution\n",
    "def analyze_image_quality(data_path, split='train', samples=100):\n",
    "    \"\"\"Analyze brightness, contrast, and color properties\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for emotion in EMOTIONS:\n",
    "        emotion_path = data_path / split / emotion\n",
    "        \n",
    "        if not emotion_path.exists():\n",
    "            continue\n",
    "        \n",
    "        images = list(emotion_path.glob('*.jpg')) + \\\n",
    "                list(emotion_path.glob('*.png')) + \\\n",
    "                list(emotion_path.glob('*.jpeg'))\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "            \n",
    "        sample = np.random.choice(images, min(samples, len(images)), replace=False)\n",
    "        \n",
    "        for img_path in sample:\n",
    "            try:\n",
    "                img = cv2.imread(str(img_path))\n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                brightness = np.mean(img_gray)\n",
    "                contrast = np.std(img_gray)\n",
    "                \n",
    "                # Color channel means\n",
    "                r_mean = np.mean(img_rgb[:, :, 0])\n",
    "                g_mean = np.mean(img_rgb[:, :, 1])\n",
    "                b_mean = np.mean(img_rgb[:, :, 2])\n",
    "                \n",
    "                results.append({\n",
    "                    'emotion': emotion,\n",
    "                    'brightness': brightness,\n",
    "                    'contrast': contrast,\n",
    "                    'red_mean': r_mean,\n",
    "                    'green_mean': g_mean,\n",
    "                    'blue_mean': b_mean\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"üé® Analyzing image quality...\")\n",
    "df_quality = analyze_image_quality(DATA_DIR, split='train', samples=100)\n",
    "\n",
    "print(\"\\nüìä Image Quality Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBrightness (0-255):\")\n",
    "print(f\"  Mean: {df_quality['brightness'].mean():.2f}\")\n",
    "print(f\"  Std:  {df_quality['brightness'].std():.2f}\")\n",
    "print(f\"  Range: {df_quality['brightness'].min():.2f} - {df_quality['brightness'].max():.2f}\")\n",
    "\n",
    "print(f\"\\nContrast (std of pixel values):\")\n",
    "print(f\"  Mean: {df_quality['contrast'].mean():.2f}\")\n",
    "print(f\"  Std:  {df_quality['contrast'].std():.2f}\")\n",
    "print(f\"  Range: {df_quality['contrast'].min():.2f} - {df_quality['contrast'].max():.2f}\")\n",
    "\n",
    "print(f\"\\nColor Channel Means:\")\n",
    "print(f\"  Red:   {df_quality['red_mean'].mean():.2f}\")\n",
    "print(f\"  Green: {df_quality['green_mean'].mean():.2f}\")\n",
    "print(f\"  Blue:  {df_quality['blue_mean'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quality metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Image Quality Analysis', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 1. Brightness distribution by emotion\n",
    "df_quality.boxplot(column='brightness', by='emotion', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Brightness by Emotion')\n",
    "axes[0, 0].set_xlabel('Emotion')\n",
    "axes[0, 0].set_ylabel('Brightness (0-255)')\n",
    "plt.setp(axes[0, 0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. Contrast distribution by emotion\n",
    "df_quality.boxplot(column='contrast', by='emotion', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Contrast by Emotion')\n",
    "axes[0, 1].set_xlabel('Emotion')\n",
    "axes[0, 1].set_ylabel('Contrast (Std)')\n",
    "plt.setp(axes[0, 1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 3. Color distribution\n",
    "color_means = df_quality[['red_mean', 'green_mean', 'blue_mean']].mean()\n",
    "axes[1, 0].bar(['Red', 'Green', 'Blue'], color_means, color=['red', 'green', 'blue'], alpha=0.7)\n",
    "axes[1, 0].set_title('Average Color Channel Values')\n",
    "axes[1, 0].set_ylabel('Mean Value (0-255)')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Brightness vs Contrast scatter\n",
    "scatter = axes[1, 1].scatter(df_quality['brightness'], df_quality['contrast'],\n",
    "                            c=df_quality['emotion'].astype('category').cat.codes,\n",
    "                            alpha=0.5, cmap='tab10')\n",
    "axes[1, 1].set_title('Brightness vs Contrast')\n",
    "axes[1, 1].set_xlabel('Brightness')\n",
    "axes[1, 1].set_ylabel('Contrast')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/image_quality.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üíæ Saved visualization to: ../results/plots/image_quality.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® 7. Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential data issues\n",
    "print(\"üîç Checking for Data Quality Issues...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "issues = []\n",
    "\n",
    "# 1. Check for very small images\n",
    "small_images = df_images[(df_images['width'] < 48) | (df_images['height'] < 48)]\n",
    "if len(small_images) > 0:\n",
    "    issues.append(f\"‚ö†Ô∏è  Found {len(small_images)} images smaller than 48x48 pixels\")\n",
    "    print(f\"\\n{issues[-1]}\")\n",
    "    print(f\"   Consider removing or excluding these from training\")\n",
    "\n",
    "# 2. Check for extreme aspect ratios\n",
    "extreme_ar = df_images[(df_images['aspect_ratio'] < 0.5) | (df_images['aspect_ratio'] > 2.0)]\n",
    "if len(extreme_ar) > 0:\n",
    "    issues.append(f\"‚ö†Ô∏è  Found {len(extreme_ar)} images with extreme aspect ratios\")\n",
    "    print(f\"\\n{issues[-1]}\")\n",
    "    print(f\"   These might be cropped incorrectly\")\n",
    "\n",
    "# 3. Check for grayscale images in RGB dataset\n",
    "if 'channels' in df_images.columns:\n",
    "    grayscale = df_images[df_images['channels'] == 1]\n",
    "    if len(grayscale) > 0:\n",
    "        issues.append(f\"‚ö†Ô∏è  Found {len(grayscale)} grayscale images\")\n",
    "        print(f\"\\n{issues[-1]}\")\n",
    "        print(f\"   Consider converting to RGB for consistency\")\n",
    "\n",
    "# 4. Check for very dark or bright images\n",
    "if len(df_quality) > 0:\n",
    "    very_dark = df_quality[df_quality['brightness'] < 30]\n",
    "    very_bright = df_quality[df_quality['brightness'] > 225]\n",
    "    \n",
    "    if len(very_dark) > 0:\n",
    "        issues.append(f\"‚ö†Ô∏è  Found {len(very_dark)} very dark images (brightness < 30)\")\n",
    "        print(f\"\\n{issues[-1]}\")\n",
    "    \n",
    "    if len(very_bright) > 0:\n",
    "        issues.append(f\"‚ö†Ô∏è  Found {len(very_bright)} very bright images (brightness > 225)\")\n",
    "        print(f\"\\n{issues[-1]}\")\n",
    "\n",
    "# 5. Check for low contrast images\n",
    "if len(df_quality) > 0:\n",
    "    low_contrast = df_quality[df_quality['contrast'] < 20]\n",
    "    if len(low_contrast) > 0:\n",
    "        issues.append(f\"‚ö†Ô∏è  Found {len(low_contrast)} low contrast images (std < 20)\")\n",
    "        print(f\"\\n{issues[-1]}\")\n",
    "        print(f\"   These might be blurry or have poor quality\")\n",
    "\n",
    "if len(issues) == 0:\n",
    "    print(\"\\n‚úÖ No major data quality issues detected!\")\n",
    "else:\n",
    "    print(f\"\\n\\nüìã Summary: Found {len(issues)} potential issues\")\n",
    "    print(\"   Review these and consider preprocessing steps to address them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù 8. Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä KEY FINDINGS & RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  DATASET SIZE:\")\n",
    "for split, info in dataset_info.items():\n",
    "    print(f\"   - {split.upper()}: {info['total']:,} images\")\n",
    "total = sum(info['total'] for info in dataset_info.values())\n",
    "print(f\"   - TOTAL: {total:,} images\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  CLASS BALANCE:\")\n",
    "for split, info in dataset_info.items():\n",
    "    counts = list(info['classes'].values())\n",
    "    if counts:\n",
    "        imbalance_ratio = max(counts) / min(counts) if min(counts) > 0 else float('inf')\n",
    "        print(f\"   - {split.upper()}: {imbalance_ratio:.2f}:1 ratio\")\n",
    "        if imbalance_ratio > 2:\n",
    "            print(f\"     ‚ö†Ô∏è  RECOMMENDATION: Use class weights or focal loss\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  IMAGE PROPERTIES:\")\n",
    "print(f\"   - Size range: {df_images['width'].min():.0f}x{df_images['height'].min():.0f} to \"\n",
    "      f\"{df_images['width'].max():.0f}x{df_images['height'].max():.0f}\")\n",
    "print(f\"   - Average size: {df_images['width'].mean():.0f}x{df_images['height'].mean():.0f}\")\n",
    "print(f\"   ‚úÖ RECOMMENDATION: Resize all images to 224x224 for training\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  DATA AUGMENTATION:\")\n",
    "print(f\"   ‚úÖ RECOMMENDED techniques:\")\n",
    "print(f\"      - Random horizontal flips (faces are mostly symmetric)\")\n",
    "print(f\"      - Small rotations (¬±15¬∞)\")\n",
    "print(f\"      - Brightness/contrast adjustments\")\n",
    "print(f\"      - Random crops\")\n",
    "print(f\"   ‚ö†Ô∏è  AVOID:\")\n",
    "print(f\"      - Vertical flips (unnatural)\")\n",
    "print(f\"      - Large rotations (>20¬∞)\")\n",
    "print(f\"      - Heavy color distortions\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£  PREPROCESSING PIPELINE:\")\n",
    "print(f\"   1. Resize to 224x224\")\n",
    "print(f\"   2. Convert to RGB (if needed)\")\n",
    "print(f\"   3. Normalize with ImageNet stats\")\n",
    "print(f\"   4. Apply augmentation (training only)\")\n",
    "\n",
    "print(\"\\n6Ô∏è‚É£  MODEL RECOMMENDATIONS:\")\n",
    "print(f\"   - Start with: Baseline CNN (fast iteration)\")\n",
    "print(f\"   - Best results: EfficientNet-B0 (pretrained)\")\n",
    "print(f\"   - Alternative: ResNet50 (proven architecture)\")\n",
    "\n",
    "print(\"\\n7Ô∏è‚É£  TRAINING STRATEGY:\")\n",
    "if any('imbalance' in str(i).lower() for i in issues):\n",
    "    print(f\"   ‚ö†Ô∏è  Due to class imbalance:\")\n",
    "    print(f\"      - Use weighted loss or focal loss\")\n",
    "    print(f\"      - Monitor per-class metrics\")\n",
    "    print(f\"      - Consider oversampling minority classes\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Classes are balanced, standard training should work well\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ EDA COMPLETE - Ready to start preprocessing and training!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 9. Save Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary statistics to file\n",
    "summary_path = Path('../results/reports/eda_summary.txt')\n",
    "summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"FACIAL EMOTION RECOGNITION - EDA SUMMARY\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATASET STATISTICS:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    for split, info in dataset_info.items():\n",
    "        f.write(f\"\\n{split.upper()} SET: {info['total']:,} images\\n\")\n",
    "        for emotion, count in info['classes'].items():\n",
    "            percentage = (count / info['total'] * 100) if info['total'] > 0 else 0\n",
    "            f.write(f\"  {emotion:12s}: {count:5,} ({percentage:5.2f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "    f.write(f\"TOTAL: {sum(info['total'] for info in dataset_info.values()):,} images\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\nIMAGE PROPERTIES:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(f\"Width:  {df_images['width'].min():.0f} - {df_images['width'].max():.0f} px\\n\")\n",
    "    f.write(f\"Height: {df_images['height'].min():.0f} - {df_images['height'].max():.0f} px\\n\")\n",
    "    f.write(f\"Average: {df_images['width'].mean():.0f}x{df_images['height'].mean():.0f}\\n\")\n",
    "    \n",
    "    if len(issues) > 0:\n",
    "        f.write(\"\\n\\nDATA QUALITY ISSUES:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for issue in issues:\n",
    "            f.write(f\"{issue}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\\nRECOMMENDATIONS:\\n\")\n",
    "    f.write(\"-\"*70 + \"\\n\")\n",
    "    f.write(\"1. Resize all images to 224x224\\n\")\n",
    "    f.write(\"2. Use ImageNet normalization\\n\")\n",
    "    f.write(\"3. Apply data augmentation\\n\")\n",
    "    f.write(\"4. Start with EfficientNet-B0\\n\")\n",
    "    f.write(\"5. Monitor class-wise metrics\\n\")\n",
    "\n",
    "print(f\"\\nüíæ Saved EDA summary to: {summary_path}\")\n",
    "\n",
    "# Save DataFrames\n",
    "df_stats.to_csv('../results/metrics/class_distribution.csv', index=False)\n",
    "df_images.to_csv('../results/metrics/image_properties.csv', index=False)\n",
    "if len(df_quality) > 0:\n",
    "    df_quality.to_csv('../results/metrics/image_quality.csv', index=False)\n",
    "\n",
    "print(\"üíæ Saved CSV files to: ../results/metrics/\")\n",
    "print(\"\\n‚úÖ EDA notebook complete! You're ready to move to preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Based on this analysis:\n",
    "\n",
    "1. **Preprocessing** (`02_preprocessing.ipynb`)\n",
    "   - Implement image resizing\n",
    "   - Setup data augmentation\n",
    "   - Create train/val/test splits\n",
    "\n",
    "2. **Baseline Model** (`03_baseline_model.ipynb`)\n",
    "   - Build simple CNN\n",
    "   - Establish baseline performance\n",
    "   - Identify areas for improvement\n",
    "\n",
    "3. **Advanced Models** (`04_model_experiments.ipynb`)\n",
    "   - Transfer learning (ResNet, EfficientNet)\n",
    "   - Hyperparameter tuning\n",
    "   - Ensemble methods\n",
    "\n",
    "Good luck! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
