"""
Data splitter for creating stratified train/val/test splits.
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, Optional, Dict
from sklearn.model_selection import train_test_split
import yaml


class DataSplitter:
    """
    Creates stratified train/val/test splits from image dataset.
    
    Ensures:
    - Stratified splitting (maintains class distribution)
    - Reproducibility (fixed random seed)
    - Proper split ratios
    - Verification of distributions
    """
    
    def __init__(
        self,
        train_ratio: float = 0.8,
        val_ratio: float = 0.1,
        test_ratio: float = 0.1,
        random_seed: int = 42
    ):
        """
        Initialize the data splitter.
        
        Args:
            train_ratio: Proportion of data for training (default: 0.8)
            val_ratio: Proportion of data for validation (default: 0.1)
            test_ratio: Proportion of data for testing (default: 0.1)
            random_seed: Random seed for reproducibility (default: 42)
        """
        # Validate ratios
        total = train_ratio + val_ratio + test_ratio
        if not np.isclose(total, 1.0):
            raise ValueError(f"Split ratios must sum to 1.0, got {total}")
        
        self.train_ratio = train_ratio
        self.val_ratio = val_ratio
        self.test_ratio = test_ratio
        self.random_seed = random_seed
        
        # Set random seed
        np.random.seed(random_seed)
        
        # Store split results
        self.train_df = None
        self.val_df = None
        self.test_df = None
    
    def split_data(
        self,
        df: pd.DataFrame,
        stratify_column: str = 'emotion'
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Create stratified train/val/test splits.
        
        Args:
            df: DataFrame with image paths and labels
            stratify_column: Column name to stratify on (default: 'emotion')
            
        Returns:
            Tuple of (train_df, val_df, test_df)
        """
        print(f"Splitting {len(df)} images...")
        print(f"Ratios - Train: {self.train_ratio}, Val: {self.val_ratio}, Test: {self.test_ratio}")
        
        # First split: train vs (val + test)
        train_df, temp_df = train_test_split(
            df,
            test_size=(self.val_ratio + self.test_ratio),
            stratify=df[stratify_column],
            random_state=self.random_seed
        )
        
        # Second split: val vs test
        # Calculate the proportion of validation in the temp set
        val_size = self.val_ratio / (self.val_ratio + self.test_ratio)
        
        val_df, test_df = train_test_split(
            temp_df,
            test_size=(1 - val_size),
            stratify=temp_df[stratify_column],
            random_state=self.random_seed
        )
        
        # Store results
        self.train_df = train_df
        self.val_df = val_df
        self.test_df = test_df
        
        print(f"\nâœ… Split complete:")
        print(f"  Train: {len(train_df):,} images ({len(train_df)/len(df)*100:.1f}%)")
        print(f"  Val:   {len(val_df):,} images ({len(val_df)/len(df)*100:.1f}%)")
        print(f"  Test:  {len(test_df):,} images ({len(test_df)/len(df)*100:.1f}%)")
        
        return train_df, val_df, test_df
    
    def verify_splits(
        self,
        stratify_column: str = 'emotion',
        tolerance: float = 0.02
    ) -> pd.DataFrame:
        """
        Verify that splits maintain class distributions.
        
        Args:
            stratify_column: Column to check distribution
            tolerance: Acceptable deviation from target ratio (default: 0.02 = 2%)
            
        Returns:
            DataFrame with distribution verification
        """
        if self.train_df is None:
            raise ValueError("No splits available. Run split_data() first.")
        
        # Create verification DataFrame
        verification = pd.DataFrame({
            'train': self.train_df[stratify_column].value_counts().sort_index(),
            'val': self.val_df[stratify_column].value_counts().sort_index(),
            'test': self.test_df[stratify_column].value_counts().sort_index()
        }).fillna(0).astype(int)
        
        # Add totals and percentages
        verification['total'] = verification.sum(axis=1)
        verification['train_%'] = (verification['train'] / verification['total'] * 100).round(1)
        verification['val_%'] = (verification['val'] / verification['total'] * 100).round(1)
        verification['test_%'] = (verification['test'] / verification['total'] * 100).round(1)
        
        # Verify ratios are within tolerance
        target_train = self.train_ratio * 100
        target_val = self.val_ratio * 100
        target_test = self.test_ratio * 100
        
        train_ok = all(abs(verification['train_%'] - target_train) <= tolerance * 100)
        val_ok = all(abs(verification['val_%'] - target_val) <= tolerance * 100)
        test_ok = all(abs(verification['test_%'] - target_test) <= tolerance * 100)
        
        print("\nðŸ“Š Split Distribution Verification:")
        print(verification)
        
        if train_ok and val_ok and test_ok:
            print(f"\nâœ… All splits within {tolerance*100}% tolerance")
        else:
            print(f"\nâš ï¸  Warning: Some splits exceed {tolerance*100}% tolerance")
        
        return verification
    
    def load_from_directory(
        self,
        data_dir: str,
        extensions: Tuple[str, ...] = ('.png', '.jpg', '.jpeg')
    ) -> pd.DataFrame:
        """
        Load image paths from directory structure.
        
        Expected structure:
        data_dir/
        â”œâ”€â”€ emotion1/
        â”‚   â”œâ”€â”€ image1.png
        â”‚   â””â”€â”€ image2.png
        â””â”€â”€ emotion2/
            â””â”€â”€ image3.png
        
        Args:
            data_dir: Root directory containing emotion subdirectories
            extensions: Tuple of valid image extensions
            
        Returns:
            DataFrame with columns ['path', 'emotion', 'filename']
        """
        data_dir = Path(data_dir)
        
        if not data_dir.exists():
            raise ValueError(f"Directory not found: {data_dir}")
        
        records = []
        
        # Get all emotion directories
        emotion_dirs = [d for d in data_dir.iterdir() if d.is_dir()]
        
        print(f"Scanning {len(emotion_dirs)} emotion directories...")
        
        for emotion_dir in sorted(emotion_dirs):
            emotion = emotion_dir.name
            
            # Get all images in this emotion directory
            for ext in extensions:
                for img_path in emotion_dir.glob(f'*{ext}'):
                    records.append({
                        'path': str(img_path),
                        'emotion': emotion,
                        'filename': img_path.name
                    })
        
        df = pd.DataFrame(records)
        
        print(f"\nâœ… Loaded {len(df):,} images")
        print(f"\nClass distribution:")
        print(df['emotion'].value_counts().sort_index())
        
        return df
    
    def save_splits(
        self,
        output_dir: str,
        save_info: bool = True
    ) -> None:
        """
        Save split DataFrames to CSV files.
        
        Args:
            output_dir: Directory to save split files
            save_info: Whether to save split metadata (default: True)
        """
        if self.train_df is None:
            raise ValueError("No splits available. Run split_data() first.")
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save DataFrames
        self.train_df.to_csv(output_dir / 'train_split.csv', index=False)
        self.val_df.to_csv(output_dir / 'val_split.csv', index=False)
        self.test_df.to_csv(output_dir / 'test_split.csv', index=False)
        
        print(f"\nâœ… Saved split files to: {output_dir}")
        print(f"  - train_split.csv ({len(self.train_df):,} rows)")
        print(f"  - val_split.csv ({len(self.val_df):,} rows)")
        print(f"  - test_split.csv ({len(self.test_df):,} rows)")
        
        # Save split metadata
        if save_info:
            split_info = {
                'random_seed': self.random_seed,
                'train_ratio': float(self.train_ratio),
                'val_ratio': float(self.val_ratio),
                'test_ratio': float(self.test_ratio),
                'train_count': len(self.train_df),
                'val_count': len(self.val_df),
                'test_count': len(self.test_df),
                'total_count': len(self.train_df) + len(self.val_df) + len(self.test_df)
            }
            
            info_path = output_dir / 'split_info.yaml'
            with open(info_path, 'w') as f:
                yaml.dump(split_info, f, default_flow_style=False)
            
            print(f"  - split_info.yaml")
    
    def load_splits(self, input_dir: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        Load previously saved splits.
        
        Args:
            input_dir: Directory containing split CSV files
            
        Returns:
            Tuple of (train_df, val_df, test_df)
        """
        input_dir = Path(input_dir)
        
        self.train_df = pd.read_csv(input_dir / 'train_split.csv')
        self.val_df = pd.read_csv(input_dir / 'val_split.csv')
        self.test_df = pd.read_csv(input_dir / 'test_split.csv')
        
        print(f"âœ… Loaded splits from: {input_dir}")
        print(f"  Train: {len(self.train_df):,} images")
        print(f"  Val:   {len(self.val_df):,} images")
        print(f"  Test:  {len(self.test_df):,} images")
        
        return self.train_df, self.val_df, self.test_df


if __name__ == '__main__':
    # Example usage
    splitter = DataSplitter(
        train_ratio=0.8,
        val_ratio=0.1,
        test_ratio=0.1,
        random_seed=42
    )
    
    # Load data from directory
    df = splitter.load_from_directory('data/raw')
    
    # Create splits
    train_df, val_df, test_df = splitter.split_data(df)
    
    # Verify splits
    verification = splitter.verify_splits()
    
    # Save splits
    splitter.save_splits('results/metrics')